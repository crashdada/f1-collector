#!/usr/bin/env python3
"""
F1 NAS Syncer â€” å°†é‡‡é›†åˆ°çš„æ•°æ®åŒæ­¥è‡³å±•ç¤ºç«™ç›®å½•

è‡ªåŠ¨æ£€æµ‹ç¯å¢ƒï¼š
  - æœ¬åœ°å¼€å‘ï¼ˆ../f1-website/package.json å­˜åœ¨ï¼‰â†’ å†™å…¥ public/data/ å’Œ public/
  - NAS éƒ¨ç½²ï¼ˆæ—  package.jsonï¼‰â†’ å†™å…¥ data/ å’Œæ ¹ç›®å½•

NAS ç›®å½•ç»“æ„ï¼š                    æœ¬åœ°ç›®å½•ç»“æ„ï¼š
  web/                             oc/
  â”œâ”€â”€ f1-collector/                â”œâ”€â”€ f1-collector/
  â”‚   â”œâ”€â”€ syncer.py                â”‚   â”œâ”€â”€ syncer.py
  â”‚   â””â”€â”€ *.json                   â”‚   â””â”€â”€ *.json
  â””â”€â”€ f1-website/  (dist)          â””â”€â”€ f1-website/  (æºç )
      â”œâ”€â”€ index.html                   â”œâ”€â”€ package.json
      â”œâ”€â”€ f1.db                        â”œâ”€â”€ public/
      â””â”€â”€ data/*.json                  â”‚   â”œâ”€â”€ f1.db
                                       â”‚   â””â”€â”€ data/*.json
                                       â””â”€â”€ src/

ç”¨æ³•ï¼š
  python syncer.py               # åŒæ­¥æ‰€æœ‰ JSON
  python syncer.py --schedule    # ä»…åŒæ­¥èµ›ç¨‹
  python syncer.py --db          # åŒæ­¥æ•°æ®åº“ï¼ˆf1.dbï¼‰
  python syncer.py --all         # åŒæ­¥ JSON + DB
  python syncer.py --scrape      # å…ˆè¿è¡Œ scraper.py å†åŒæ­¥
"""

import json
import os
import shutil
import argparse
import subprocess
import sys
from datetime import datetime

# è·¯å¾„è®¡ç®—
COLLECTOR_DIR = os.path.dirname(os.path.abspath(__file__))
PARENT_DIR = os.path.dirname(COLLECTOR_DIR)
WEBSITE_DIR = os.path.join(PARENT_DIR, 'f1-website')

# ç¯å¢ƒæ£€æµ‹ï¼šæœ‰ package.json è¯´æ˜æ˜¯æœ¬åœ°æºç ç¯å¢ƒ
IS_LOCAL = os.path.exists(os.path.join(WEBSITE_DIR, 'package.json'))

if IS_LOCAL:
    # æœ¬åœ°å¼€å‘ï¼šJSON â†’ public/data/ï¼ŒDB â†’ public/
    JSON_SOURCE = os.path.join(COLLECTOR_DIR, 'data')
    JSON_TARGET = os.path.join(WEBSITE_DIR, 'public', 'data')
    DB_TARGET = os.path.join(WEBSITE_DIR, 'public')
    ENV_NAME = 'æœ¬åœ°å¼€å‘'
else:
    # NAS éƒ¨ç½²ï¼šæ£€æµ‹æ˜¯å¦æœ‰ dist ç›®å½•ï¼ˆé’ˆå¯¹åªéƒ¨ç½²ç”Ÿæˆçš„ç½‘é¡µå†…å®¹çš„æƒ…å†µï¼‰
    potential_dist = os.path.join(WEBSITE_DIR, 'dist')
    DEPLOY_ROOT = potential_dist if os.path.exists(potential_dist) else WEBSITE_DIR
    
    JSON_SOURCE = os.path.join(COLLECTOR_DIR, 'data')
    JSON_TARGET = os.path.join(DEPLOY_ROOT, 'data')
    DB_TARGET = DEPLOY_ROOT
    ENV_NAME = f'NAS éƒ¨ç½² (ç›®æ ‡: {os.path.basename(DEPLOY_ROOT)})'

# è¦åŒæ­¥çš„ JSON æ–‡ä»¶
JSON_FILES = [
    'schedule_2026.json',
    'drivers_2026.json',
    'teams_2026.json',
]


def log(msg):
    ts = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print(f'[{ts}] {msg}')


def sync_json(filename):
    """åŒæ­¥å•ä¸ª JSON æ–‡ä»¶"""
    source = os.path.join(JSON_SOURCE, filename)
    target = os.path.join(JSON_TARGET, filename)

    if not os.path.exists(source):
        log(f'âš  æºæ–‡ä»¶ä¸å­˜åœ¨: {source}')
        return False

    os.makedirs(JSON_TARGET, exist_ok=True)

    # éªŒè¯ JSON åˆæ³•æ€§
    try:
        with open(source, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except json.JSONDecodeError as e:
        log(f'âœ— JSON æ ¼å¼é”™è¯¯ {filename}: {e}')
        return False

    # è·³è¿‡æ— å˜æ›´çš„æ–‡ä»¶
    if os.path.exists(target):
        with open(target, 'r', encoding='utf-8') as f:
            try:
                if json.load(f) == data:
                    log(f'â€¢ {filename} æ— å˜æ›´ï¼Œè·³è¿‡')
                    return True
            except json.JSONDecodeError:
                pass

    shutil.copy2(source, target)
    count = len(data) if isinstance(data, list) else 'object'
    log(f'âœ“ {filename} ({count} entries) â†’ {target}')
    return True


def sync_db():
    """åŒæ­¥ f1.db"""
    source = os.path.join(COLLECTOR_DIR, 'f1.db')
    target = os.path.join(DB_TARGET, 'f1.db')

    if not os.path.exists(source):
        log(f'âš  æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {source}')
        return False

    if os.path.exists(target):
        if os.path.getsize(source) == os.path.getsize(target):
            log(f'â€¢ f1.db å¤§å°æœªå˜ ({os.path.getsize(source)} bytes)ï¼Œè·³è¿‡')
            return True

    shutil.copy2(source, target)
    log(f'âœ“ f1.db ({os.path.getsize(target):,} bytes) â†’ {target}')
    return True


def run_scraper():
    """è¿è¡Œ scraper.py"""
    scraper = os.path.join(COLLECTOR_DIR, 'scraper.py')
    if not os.path.exists(scraper):
        log('âœ— scraper.py ä¸å­˜åœ¨')
        return False

    log('ğŸ”„ è¿è¡Œ scraper.py ...')
    result = subprocess.run(
        [sys.executable, scraper],
        cwd=COLLECTOR_DIR,
        capture_output=True,
        text=True
    )

    if result.returncode != 0:
        log(f'âœ— scraper.py å¤±è´¥:\n{result.stderr}')
        return False

    if result.stdout.strip():
        for line in result.stdout.strip().split('\n'):
            log(f'  scraper: {line}')
    log('âœ“ scraper.py å®Œæˆ')
    return True


def main():
    parser = argparse.ArgumentParser(description='F1 æ•°æ®åŒæ­¥å·¥å…·')
    parser.add_argument('--schedule', action='store_true', help='ä»…åŒæ­¥èµ›ç¨‹')
    parser.add_argument('--db', action='store_true', help='åŒæ­¥ f1.db')
    parser.add_argument('--all', action='store_true', help='JSON + DB å…¨éƒ¨åŒæ­¥')
    parser.add_argument('--scrape', action='store_true', help='å…ˆé‡‡é›†å†åŒæ­¥')
    args = parser.parse_args()

    log('=' * 50)
    log(f'F1 Syncer | ç¯å¢ƒ: {ENV_NAME}')
    log(f'é‡‡é›†ç«¯: {COLLECTOR_DIR}')
    log(f'JSON â†  {JSON_SOURCE}')
    log(f'JSON â†’  {JSON_TARGET}')
    log('=' * 50)

    if not os.path.exists(WEBSITE_DIR):
        log(f'âœ— å±•ç¤ºç«™ç›®å½•ä¸å­˜åœ¨: {WEBSITE_DIR}')
        sys.exit(1)

    if args.scrape:
        if not run_scraper():
            log('âš  é‡‡é›†å¤±è´¥ï¼Œç»§ç»­åŒæ­¥å·²æœ‰æ•°æ®...')

    if args.schedule:
        sync_json('schedule_2026.json')
    elif args.db:
        sync_db()
    elif args.all:
        for f in JSON_FILES:
            sync_json(f)
        sync_db()
    else:
        for f in JSON_FILES:
            sync_json(f)

    log('åŒæ­¥å®Œæˆ âœ…')


if __name__ == '__main__':
    main()
