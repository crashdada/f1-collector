#!/usr/bin/env python3
"""
F1 2026 Race Results Scraper â€” èµ›åæˆç»©é‡‡é›†æ¡†æ¶

åŠŸèƒ½ï¼š
  - æ£€æµ‹èµ›åçª—å£ï¼ˆæ¯”èµ›ç»“æŸå 1-3 å¤©è‡ªåŠ¨è§¦å‘ï¼‰
  - ä» F1 å®˜ç½‘é‡‡é›†æ¯”èµ›æˆç»©
  - è¾“å‡ºç»“æ„åŒ– JSONï¼Œä¾› syncer.py åŒæ­¥åˆ°å±•ç¤ºç«¯

ç”¨æ³•ï¼š
  python scraper_results_2026.py                  # è‡ªåŠ¨æ£€æµ‹çª—å£
  python scraper_results_2026.py --force           # å¼ºåˆ¶é‡‡é›†æœ€è¿‘ä¸€åœº
  python scraper_results_2026.py --round 3         # é‡‡é›†æŒ‡å®šè½®æ¬¡

æ³¨æ„ï¼š2026 èµ›å­£å°šæœªå¼€å§‹ï¼Œæœ¬è„šæœ¬ä¸ºé¢„å¤‡æ¡†æ¶ã€‚
      å®é™…è§£æé€»è¾‘éœ€æ ¹æ®èµ›å­£å¼€å§‹åçš„é¡µé¢ç»“æ„å¾®è°ƒã€‚
"""

import json
import os
import re
import sys
import datetime
import argparse

# å¤ç”¨ scraper.py çš„æ ¸å¿ƒèƒ½åŠ›
from scraper import F1DataCollector


SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
SCHEDULE_FILE = os.path.join(SCRIPT_DIR, 'data', 'schedule_2026.json')
RESULTS_DIR = os.path.join(SCRIPT_DIR, 'results_2026')


def load_schedule():
    """åŠ è½½èµ›å†"""
    if not os.path.exists(SCHEDULE_FILE):
        print(f'[!] èµ›å†ä¸å­˜åœ¨: {SCHEDULE_FILE}')
        return []
    with open(SCHEDULE_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)


def find_recent_race(schedule):
    """æ‰¾åˆ°æœ€è¿‘ç»“æŸçš„æ¯”èµ›ï¼ˆèµ›å 1-3 å¤©å†…ï¼‰"""
    today = datetime.date.today()
    months = {
        'JAN': 1, 'FEB': 2, 'MAR': 3, 'APR': 4, 'MAY': 5, 'JUN': 6,
        'JUL': 7, 'AUG': 8, 'SEP': 9, 'OCT': 10, 'NOV': 11, 'DEC': 12
    }

    for race in schedule:
        if race.get('isTest', False):
            continue

        dates = race.get('dates', '')
        match = re.search(r'-\s+(\d+)\s+([A-Z]{3})', dates)
        if not match:
            continue

        day = int(match.group(1))
        month = months.get(match.group(2).upper())
        if not month:
            continue

        try:
            race_end = datetime.date(2026, month, day)
            delta = (today - race_end).days
            if 0 <= delta <= 3:
                return race
        except ValueError:
            continue

    return None


def find_race_by_round(schedule, round_num):
    """æŒ‰è½®æ¬¡æŸ¥æ‰¾æ¯”èµ›"""
    for race in schedule:
        round_text = race.get('round', '')
        match = re.search(r'(\d+)', round_text)
        if match and int(match.group(1)) == round_num:
            return race
    return None


def scrape_race_results(collector, race):
    """
    é‡‡é›†å•åœºæ¯”èµ›æˆç»©
    
    TODO: 2026 èµ›å­£å¼€å§‹åï¼Œæ ¹æ®å®é™…é¡µé¢ç»“æ„å®Œå–„è§£æé€»è¾‘
    ç›®å‰ä½¿ç”¨ scraper.py ä¸­çš„ get_race_results() ä½œä¸ºåŸºç¡€
    """
    slug = race.get('slug', '')
    country = race.get('country', race.get('location', 'unknown'))
    round_text = race.get('round', '')

    # æ„é€ ç»“æœé¡µ URL
    # æ ¼å¼å¯èƒ½ä¸º: /en/results/2026/races/round-X/slug/race-result
    result_url = f"{collector.base_url}/en/results/2026/races/{slug}/race-result"

    print(f'Race: é‡‡é›†: {country} ({round_text})')
    print(f'   URL: {result_url}')

    # è·å–é¡µé¢ HTML
    html = collector.fetch_page(result_url, max_retries=2, initial_delay=60)
    if not html:
        print(f'   [!] é¡µé¢è·å–å¤±è´¥')
        return None

    # è§£ææˆç»©ï¼ˆç›´æ¥ä¼ å…¥å·²æœ‰ HTMLï¼Œé¿å…äºŒæ¬¡ fetchï¼‰
    results = collector.get_race_results(html)
    if not results:
        print(f'   [!] æœªæ‰¾åˆ°æˆç»©æ•°æ®ï¼ˆé¡µé¢ç»“æ„å¯èƒ½å·²å˜åŒ–ï¼‰')
        return None

    # æ„é€ è¾“å‡º
    output = {
        'round': round_text,
        'country': country,
        'slug': slug,
        'scraped_at': datetime.datetime.now().isoformat(),
        'results': results
    }

    print(f'   [OK] è·å–åˆ° {len(results)} æ¡æˆç»©')
    return output


def save_results(data, race):
    """ä¿å­˜é‡‡é›†ç»“æœ"""
    os.makedirs(RESULTS_DIR, exist_ok=True)

    slug = race.get('slug', 'unknown')
    filename = f'{slug}_results.json'
    filepath = os.path.join(RESULTS_DIR, filename)

    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

    print(f'   ğŸ’¾ ä¿å­˜è‡³ {filepath}')
    return filepath


def main():
    parser = argparse.ArgumentParser(description='F1 2026 èµ›åæˆç»©é‡‡é›†')
    parser.add_argument('--force', action='store_true',
                        help='å¼ºåˆ¶é‡‡é›†æœ€è¿‘ä¸€åœºï¼ˆå¿½ç•¥çª—å£æ£€æµ‹ï¼‰')
    parser.add_argument('--round', type=int,
                        help='é‡‡é›†æŒ‡å®šè½®æ¬¡çš„æˆç»©')
    args = parser.parse_args()

    print('=' * 50)
    print('F1 2026 Results Scraper')
    print('=' * 50)

    schedule = load_schedule()
    if not schedule:
        sys.exit(1)

    collector = F1DataCollector(season=2026)

    # ç¡®å®šç›®æ ‡æ¯”èµ›
    if args.round:
        race = find_race_by_round(schedule, args.round)
        if not race:
            print(f'[!] æœªæ‰¾åˆ°ç¬¬ {args.round} è½®æ¯”èµ›')
            sys.exit(1)
    elif args.force:
        # å–æœ€è¿‘çš„éæµ‹è¯•èµ›äº‹
        races = [r for r in schedule if not r.get('isTest', False)]
        race = races[-1] if races else None
        if not race:
            print('[!] èµ›å†ä¸­æ— æœ‰æ•ˆèµ›äº‹')
            sys.exit(1)
    else:
        race = find_recent_race(schedule)
        if not race:
            print('ä»Šå¤©ä¸åœ¨èµ›åçª—å£å†…ï¼Œæ— éœ€é‡‡é›†ã€‚')
            print('ä½¿ç”¨ --force æˆ– --round N å¼ºåˆ¶æ‰§è¡Œã€‚')
            sys.exit(0)

    # é‡‡é›†
    results = scrape_race_results(collector, race)
    if results:
        save_results(results, race)
        print('\né‡‡é›†å®Œæˆ [OK]')
    else:
        print('\né‡‡é›†å¤±è´¥ [!]')
        sys.exit(1)


if __name__ == '__main__':
    main()
