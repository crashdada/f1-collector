# AGENTS.md - Coding Guidelines for F1 Collector

## Build/Test/Lint Commands

```bash
# Install dependencies
pip install -r requirements.txt

# Run main scraper (Core Workflow)
python scraper.py

# Sync data to web project (auto-detects local vs NAS)
python syncer.py

# Sync with options
python syncer.py --schedule    # Schedule JSON only
python syncer.py --db           # Database only
python syncer.py --all          # JSON + DB
python syncer.py --scrape       # Run scraper first, then sync
```

## Directory Structure
```
f1-collector/
â”œâ”€â”€ scraper.py                    # èµ›å†é‡‡é›† â†’ data/schedule_2026.json
â”œâ”€â”€ scraper_drivers_2026.py       # è½¦æ‰‹æ•°æ®ç”Ÿæˆ â†’ data/drivers_2026.json
â”œâ”€â”€ scraper_teams_2026.py         # è½¦é˜Ÿæ•°æ®ç”Ÿæˆ â†’ data/teams_2026.json
â”œâ”€â”€ scraper_results_2026.py       # èµ›åæˆç»©é‡‡é›†æ¡†æ¶ï¼ˆå¾…å®æµ‹ï¼‰
â”œâ”€â”€ syncer.py                     # ç»Ÿä¸€åŒæ­¥ï¼ˆè‡ªåŠ¨æ£€æµ‹æœ¬åœ°/NASï¼‰
â”œâ”€â”€ refine_with_stats.py          # æ•°æ®å¢å¼ºè¾…åŠ©
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ AGENTS.md / WORKFLOW.md / README.md
â”œâ”€â”€ data/                         # ğŸ“¦ é‡‡é›†äº§ç‰©ï¼ˆè„šæœ¬ä¸æ•°æ®åˆ†ç¦»ï¼‰
â”‚   â”œâ”€â”€ schedule_2026.json
â”‚   â”œâ”€â”€ drivers_2026.json
â”‚   â””â”€â”€ teams_2026.json
â”œâ”€â”€ results_2026/                 # èµ›åæˆç»©ï¼ˆå¾…èµ›å­£å¼€å§‹ï¼‰
â”œâ”€â”€ baseline_2026-02-12/          # æ•°æ®åŸºçº¿å¿«ç…§
â”œâ”€â”€ assets/flags/                 # å›½æ—— SVGï¼ˆ56x56ï¼‰
â”œâ”€â”€ maintenance/
â”‚   â””â”€â”€ standardize_svgs.py
â”œâ”€â”€ research/
â”‚   â”œâ”€â”€ final_refine_2026.py
â”‚   â”œâ”€â”€ extract_rsc_2026.py
â”‚   â””â”€â”€ debug_2026.html
â””â”€â”€ .github/workflows/
    â””â”€â”€ scrape.yml                # GitHub Actionsï¼šæ¯æ—¥è‡ªåŠ¨é‡‡é›†
```

## Data Architecture
- è„šæœ¬åœ¨æ ¹ç›®å½•ï¼Œäº§ç‰©åœ¨ `data/` â€” **è„šæœ¬ä¸æ•°æ®åˆ†ç¦»**
- `syncer.py` ä» `data/` è¯»å– JSONï¼ŒåŒæ­¥åˆ°å±•ç¤ºç«¯
- å±•ç¤ºç«¯é€šè¿‡è¿è¡Œæ—¶ `fetch()` åŠ è½½ï¼Œ**ä¸æ‰“åŒ…è¿› JS**ï¼Œæ”¯æŒçƒ­æ›´æ–°

## Code Style Guidelines

### Python Conventions
- **Naming**: snake_case for functions/variables, PascalCase for classes, UPPER_CASE for constants
- **Indentation**: 4 spaces (no tabs)
- **Line length**: ~100 characters (soft limit)
- **Quotes**: Use single quotes for strings unless escaping is needed

### Imports
- Group imports: stdlib first, third-party second, local last
- Each group separated by a blank line
- Example:
  ```python
  import json
  import re
  import time
  
  import requests
  from bs4 import BeautifulSoup
  ```

### Functions & Classes
- Use docstrings for public methods (English preferred, Chinese acceptable for context)
- Keep functions focused and under 50 lines when possible
- Use `_` prefix for private methods (e.g., `_reconstruct_next_data`)

### Error Handling
- Use try/except with specific exceptions when possible
- Print errors with descriptive messages: `print(f"Error: {e}")`
- Return empty lists/None on failure rather than crashing

### Comments
- Mix of English and Chinese is acceptable (reflects project history)
- Use comments to explain WHY, not WHAT (code should be self-documenting)
- Inline comments for complex regex or parsing logic

### JSON/Data Handling
- Always use `ensure_ascii=False` when dumping JSON with non-ASCII content
- Use `encoding='utf-8'` for all file operations
- Validate data existence before accessing nested structures

### File Organization
- Keep utility scripts in root directory
- Output data files to project root
- Use descriptive filenames with timestamps when appropriate

## Git Workflow
- Automated via GitHub Actions (`.github/workflows/scrape.yml`)
- Schedule: Daily at 2 AM UTC
- Workflow commits JSON changes automatically
- Manual trigger available via `workflow_dispatch`

## Dependencies
- requests
- beautifulsoup4
- Python 3.9+ (as per CI configuration)

## Notes
- This is a data scraping project - be mindful of rate limiting
- F1 website structure may change; parsers are fragile
- Always check if data is "TBC" (To Be Confirmed) before processing
