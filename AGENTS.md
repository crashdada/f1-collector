# AGENTS.md - Coding Guidelines for F1 Collector

## Build/Test/Lint Commands

```bash
# Install dependencies
pip install -r requirements.txt

# Run main scraper (Core Workflow)
python scraper.py

# Sync data to web project (auto-detects local vs NAS)
python syncer.py

# Sync with options
python syncer.py --schedule    # Schedule JSON only
python syncer.py --db           # Database (f1.db) only
python syncer.py --all          # JSON + DB
python syncer.py --scrape       # Run scraper first, then sync
```

## Directory Structure
```
f1-collector/
â”œâ”€â”€ scraper.py                    # èµ›å†é‡‡é›† â†’ data/schedule_2026.json
â”œâ”€â”€ scraper_drivers_2026.py       # è½¦æ‰‹æ•°æ®ç”Ÿæˆ â†’ data/drivers_2026.json
â”œâ”€â”€ scraper_teams_2026.py         # è½¦é˜Ÿæ•°æ®ç”Ÿæˆ â†’ data/teams_2026.json
â”œâ”€â”€ syncer.py                     # ç»Ÿä¸€åŒæ­¥å™¨ (æ ¸å¿ƒé€»è¾‘ï¼šå®æ—¶æ›´æ–°å±•ç¤ºç«¯)
â”œâ”€â”€ refine_with_stats.py          # æ³¨å…¥å†å²ç»Ÿè®¡æ•°æ®è‡³ JSON
â”œâ”€â”€ data/                         # ğŸ“¦ é‡‡é›†äº§ç‰© (Source of Truth)
â”‚   â”œâ”€â”€ f1.db                      # å†å²æ•°æ®åº“æºæ–‡ä»¶ (1950-2025)
â”‚   â”œâ”€â”€ schedule_2026.json
â”‚   â”œâ”€â”€ drivers_2026.json
â”‚   â””â”€â”€ teams_2026.json
â”œâ”€â”€ assets/                       # è§†è§‰èµ„æºåº“ (ç”± syncer åŒæ­¥è‡³ç½‘ç«™ photos/)
â”‚   â”œâ”€â”€ seasons/2026/             # 2026 èµ›å­£èµ›è½¦ã€èµ›é“å›¾
â”‚   â””â”€â”€ flags/                    # ç»Ÿä¸€å›½æ——åº“ (56x56)
â”œâ”€â”€ photos/                       # (å¯é€‰) é¢å¤–ç…§ç‰‡å­˜å‚¨
â””â”€â”€ .github/workflows/
    â””â”€â”€ scrape.yml                # GitHub Actionsï¼šæ¯æ—¥è‡ªåŠ¨é‡‡é›†
â””â”€â”€ .github/workflows/
    â””â”€â”€ scrape.yml                # GitHub Actionsï¼šæ¯æ—¥è‡ªåŠ¨é‡‡é›†
```

## Data Architecture
- è„šæœ¬åœ¨æ ¹ç›®å½•ï¼Œäº§ç‰©åœ¨ `data/` â€” **è„šæœ¬ä¸æ•°æ®åˆ†ç¦»**
- `syncer.py` ä» `data/` è¯»å– JSON/DBï¼ŒåŒæ­¥åˆ°å±•ç¤ºç«¯çš„ `data/` ç›®å½•
- `syncer.py` ä» `assets/` è¯»å–å›¾ç‰‡ï¼ŒåŒæ­¥åˆ°å±•ç¤ºç«¯çš„ `photos/` ç›®å½• (åˆå¹¶èµ„äº§)
- å±•ç¤ºç«¯é€šè¿‡è¿è¡Œæ—¶ `fetch()` åŠ è½½ï¼Œ**æ”¯æŒ IndexedDB ç¼“å­˜**ä¸çƒ­æ›´æ–°

## Code Style Guidelines

### Python Conventions
- **Naming**: snake_case for functions/variables, PascalCase for classes, UPPER_CASE for constants
- **Indentation**: 4 spaces (no tabs)
- **Line length**: ~100 characters (soft limit)
- **Quotes**: Use single quotes for strings unless escaping is needed

### Imports
- Group imports: stdlib first, third-party second, local last
- Each group separated by a blank line
- Example:
  ```python
  import json
  import re
  import time
  
  import requests
  from bs4 import BeautifulSoup
  ```

### Functions & Classes
- Use docstrings for public methods (English preferred, Chinese acceptable for context)
- Keep functions focused and under 50 lines when possible
- Use `_` prefix for private methods (e.g., `_reconstruct_next_data`)

### Error Handling
- Use try/except with specific exceptions when possible
- Print errors with descriptive messages: `print(f"Error: {e}")`
- Return empty lists/None on failure rather than crashing

### Comments
- Mix of English and Chinese is acceptable (reflects project history)
- Use comments to explain WHY, not WHAT (code should be self-documenting)
- Inline comments for complex regex or parsing logic

### JSON/Data Handling
- Always use `ensure_ascii=False` when dumping JSON with non-ASCII content
- Use `encoding='utf-8'` for all file operations
- Validate data existence before accessing nested structures

### File Organization
- Keep utility scripts in root directory
- Output data files to project root
- Use descriptive filenames with timestamps when appropriate

## Git Workflow
- Automated via GitHub Actions (`.github/workflows/scrape.yml`)
- Schedule: Daily at 2 AM UTC
- Workflow commits JSON changes automatically
- Manual trigger available via `workflow_dispatch`

## Dependencies
- requests
- beautifulsoup4
- Python 3.9+ (as per CI configuration)

## Notes
- This is a data scraping project - be mindful of rate limiting
- F1 website structure may change; parsers are fragile
- Always check if data is "TBC" (To Be Confirmed) before processing
